# Multi-Agent Refund System with Google ADK

A production-ready **multi-agent system** built with Google's Agent Developer Kit (ADK) that automates customer refund processing using async Agent-to-Agent (A2A) communication, RAG, and LLMs.

> **Demo Use Case**: Customer support automation for "Barefoot ZÃ©nit", a premium children's shoe company.

[![Python](https://img.shields.io/badge/Python-3.10%2B-blue)](https://www.python.org/)
[![Google ADK](https://img.shields.io/badge/Google%20ADK-1.15.1-green)](https://cloud.google.com/adk)
[![Pydantic](https://img.shields.io/badge/Pydantic-2.11-orange)](https://docs.pydantic.dev/)
[![Async](https://img.shields.io/badge/100%25-Async-purple)](https://docs.python.org/3/library/asyncio.html)

---

## ğŸ¯ What It Does

This system **automatically processes refund requests** by:

1. **Classifying user intent** (refund, policy question, general query)
2. **Searching company policies** via semantic search (RAG with Firestore Vector Search)
3. **Retrieving order data** from Firestore database
4. **Validating refund eligibility** (14-day window, order status, etc.)
5. **Processing refunds** with user confirmation
6. **Assembling intelligent responses** using structured LLM outputs

**Result**: Fully automated refund workflow with human-in-the-loop confirmation for safety.

---

## ğŸ—ï¸ Architecture: Multi-Agent A2A Pattern

Instead of a monolithic agent, this system uses **specialized agents** that collaborate via standardized protocols:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       COORDINATOR AGENT (Orchestrator)       â”‚
â”‚  â€¢ Classifies intent (LLM)                   â”‚
â”‚  â€¢ Routes to specialized agents (parallel)   â”‚
â”‚  â€¢ Assembles final response (LLM)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                 â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ POLICY â”‚      â”‚TRANSACTION â”‚
â”‚ EXPERT â”‚      â”‚   AGENT    â”‚
â”‚        â”‚      â”‚            â”‚
â”‚  RAG   â”‚      â”‚ Orders DB  â”‚
â”‚ Search â”‚      â”‚ Refunds    â”‚
â”‚ (async)â”‚      â”‚Eligibility â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why Multi-Agent?

- **Scalability**: Easy to add new agents (ShippingAgent, InventoryAgent, etc.)
- **Maintainability**: Each agent is independent and testable
- **Performance**: Parallel execution via `asyncio.gather()` (500ms vs 1000ms sequential)
- **Separation of Concerns**: Policy logic â‰  Transaction logic

---

## ğŸ› ï¸ Tech Stack

| Component          | Technology                   | Purpose                                |
| ------------------ | ---------------------------- | -------------------------------------- |
| **Agent Framework**| Google ADK 1.15.1            | Multi-agent orchestration              |
| **LLM**            | Gemini 2.5 Flash             | Intent classification & response gen   |
| **Embeddings**     | Vertex AI text-embedding-004 | Vector generation for RAG              |
| **Vector DB**      | Firestore Vector Search      | Semantic search on company policies    |
| **Database**       | Firestore (AsyncClient)      | Order storage & retrieval              |
| **Validation**     | Pydantic 2.11                | Type-safe data contracts               |
| **Observability**  | Langfuse Cloud               | Complete tracing, metrics, costs       |
| **Logging**        | Structured JSON              | Production-ready parseable logs        |
| **Async Runtime**  | asyncio                      | 100% async/await (non-blocking I/O)    |

---

## âœ¨ Key Features (Production-Ready)

### ğŸš€ Performance
- **100% Async/Await**: All I/O operations are non-blocking
- **Parallel Agent Execution**: Independent tasks run concurrently (`asyncio.gather`)
- **Rate Limiting**: Shared `Semaphore` to control LLM API saturation
- **Embeddings Cache**: LRU cache reduces redundant API calls (33% hit rate)

### ğŸ”’ Reliability
- **Timeout Protection**: All LLM calls wrapped in `asyncio.wait_for(timeout=30s)`
- **Automatic Retries**: Exponential backoff via `tenacity` (3 attempts)
- **Fail-Fast Validation**: Checks ordered from fastest to slowest
- **Type Safety**: Pydantic models for ALL data contracts

### ğŸ“Š Observability
- **Distributed Tracing**: Every interaction logged to Langfuse Cloud
- **Cost Tracking**: Token usage + estimated cost per request
- **Structured Logging**: JSON logs for easy parsing and analysis
- **Performance Metrics**: Latency tracking per agent/tool
- **Context Monitoring**: Real-time token usage stats (e.g., `ğŸ’¾ Context: 12 msgs, 4523 tokens (28.3%)`)

### ğŸ§  AI/ML Best Practices
- **Structured Outputs**: LLM responses forced into Pydantic schemas (no parsing errors)
- **Externalized Prompts**: Templates in `config/prompts.yaml` with `@lru_cache`
- **RAG Pipeline**: Async embeddings â†’ Vector search â†’ Top-K ranking
- **Agent Protocols**: Standardized `AgentRequest` / `AgentResponse` for A2A communication

### ğŸ§® Context Engineering (NEW in v1.2.0)
- **Conversation History Management**: Tracks full conversation state across turns
- **Intelligent Compaction**: Automatic summarization when approaching token limits (75% threshold)
- **Token Monitoring**: Real-time token counting with `tiktoken`
- **Sliding Window**: Preserves first + last 8 messages, compresses middle messages
- **LLM Summarization**: Uses Gemini to generate concise 3-4 point summaries (70-80% compression)
- **Fallback Pruning**: Removes less relevant messages if summarization fails
- **Unlimited Conversations**: No context window overflow, supports infinite turns

---

## ğŸ”„ How It Works (Flow)

**User Input**: `"I want to return order ORD-84315"`

```
1. COORDINATOR classifies intent â†’ "refund" (LLM + structured output)

2. COORDINATOR plans agent calls â†’ [PolicyExpert, TransactionAgent] (parallel)

3. AGENTS execute in parallel:
   â”œâ”€ PolicyExpert: RAG search on refund policy (async)
   â””â”€ TransactionAgent: Fetch order from Firestore (async)

4. COORDINATOR validates eligibility â†’ TransactionAgent.check_eligibility()
   â€¢ Order status: DELIVERED âœ…
   â€¢ Days since purchase: 12 âœ… (< 14 day window)
   â€¢ Not already refunded âœ…

5. COORDINATOR assembles response (LLM + structured output)
   â†’ "Order qualifies! 2 days remaining. Reply 'yes' to confirm."

6. USER confirms â†’ TransactionAgent.process_refund()
   â†’ Updates Firestore: status="RETURNED", transaction_id="REF-1727..."
```

**Total latency**: ~1.5s (intent: 300ms, parallel agents: 500ms, eligibility: 200ms, response: 400ms)

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_agent.py           # Base class: timeout, retries, rate limiting
â”‚   â”‚   â”œâ”€â”€ coordinator.py          # Orchestrator (intent â†’ routing â†’ assembly)
â”‚   â”‚   â”œâ”€â”€ policy_expert.py        # RAG specialist
â”‚   â”‚   â””â”€â”€ transaction_agent.py    # Order & refund specialist
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ schemas.py              # Pydantic models (LLM outputs, orders, etc.)
â”‚   â”‚   â””â”€â”€ protocols.py            # A2A communication (AgentRequest/Response)
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ logger.py               # Structured JSON logging
â”‚   â”‚   â”œâ”€â”€ prompts.py              # Prompt loading with @lru_cache
â”‚   â”‚   â””â”€â”€ conversation_history.py # ğŸ†• Context engineering & history management
â”‚   â”œâ”€â”€ tools.py                    # RAG pipeline, Firestore queries
â”‚   â””â”€â”€ config.py                   # Centralized settings (Pydantic)
â”œâ”€â”€ config/
â”‚   â””â”€â”€ prompts.yaml                # Externalized LLM prompts
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ company_refund_policy_barefoot.md  # Policy document (source)
â”‚   â””â”€â”€ orders.jsonl                       # Sample orders
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 01_seed_orders.py           # Seed Firestore with orders
â”‚   â””â”€â”€ 02_setup_vector_search.py   # Generate embeddings & index
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_schemas.py             # Pydantic model validation tests
â”‚   â”œâ”€â”€ test_coordinator_extraction.py  # Order ID extraction tests
â”‚   â”œâ”€â”€ test_prompts.py             # Prompt loading tests
â”‚   â”œâ”€â”€ test_system.py              # Automated system tests (3 scenarios)
â”‚   â”œâ”€â”€ test_refund_flow.py         # End-to-end refund flow test
â”‚   â”œâ”€â”€ test_multi_agent_system.py  # Multi-agent orchestration tests
â”‚   â”œâ”€â”€ test_policy_expert.py       # RAG pipeline tests
â”‚   â”œâ”€â”€ test_transaction_agent.py   # Order & refund tests
â”‚   â””â”€â”€ test_async_tools.py         # Async tool tests
â”œâ”€â”€ main_multi_agent.py             # CLI entry point
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ CHANGELOG.md                    # Detailed version history
â”œâ”€â”€ CONTEXT_ENGINEERING_IMPROVEMENTS.md  # ğŸ†• Context management docs
â””â”€â”€ .env.example
```

---

## ğŸš€ Getting Started

### Prerequisites

- **Python 3.10+**
- **Google Cloud Project** with:
  - Firestore enabled
  - Vertex AI API enabled
- **Langfuse account** (free tier) for observability

### 1. Installation

```bash
git clone <repository-url>
cd ReAct-Google-ADK
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

### 2. Configuration

Create `.env` file:

```env
# Google Cloud
GCP_PROJECT_ID=your-project-id
GCP_LOCATION=us-central1

# AI Models
EMBEDDINGS_MODEL=text-embedding-004
AGENT_MODEL=gemini-2.5-flash

# Langfuse (Observability)
LANGFUSE_PUBLIC_KEY=pk-lf-xxxxx
LANGFUSE_SECRET_KEY=sk-lf-xxxxx
LANGFUSE_HOST=https://cloud.langfuse.com
```

### 3. Authenticate & Seed Database

```bash
# Authenticate with GCP
gcloud auth application-default login

# Load sample orders
python scripts/01_seed_orders.py

# Generate embeddings for RAG
python scripts/02_setup_vector_search.py
```

### 4. Run the System

```bash
python main_multi_agent.py
```

---

## ğŸ’¬ Usage Example

```
ğŸ¤– BAREFOOT ZÃ‰NIT - MULTI-AGENT REFUND SYSTEM
Session ID: session_abc123

âœ¨ Features:
  â€¢ Automatic refund eligibility checking
  â€¢ Confirms before processing refunds
  â€¢ Updates order status in database

ğŸ’¬ You: I want to return order ORD-84315

ğŸ”„ Processing...

ğŸ” [Consulted: policy_expert, transaction_agent]

âœ… Refund Eligible
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Your order qualifies for a refund! You purchased on
September 18, 2025 (12 days ago). Our policy allows
refunds within 14 days of delivery.

ğŸ“Œ Key Details:
  â€¢ Order: ORD-84315
  â€¢ Status: DELIVERED
  â€¢ Days remaining: 2

â¡ï¸  Next Step: Reply 'yes' to confirm refund

ğŸ’¡ Reply 'yes' to confirm refund of $89.99 for order ORD-84315

â±ï¸  Latency: 1523ms
ğŸ’¾ Context: 4 msgs, 1234 tokens (7.7%)
ğŸ“Š Trace: https://cloud.langfuse.com/project/.../traces/...

ğŸ’¬ You: yes

âœ… Refund processed successfully!
   Transaction ID: REF-1727696500000
   Amount: $89.99
   Order ORD-84315 status â†’ RETURNED
   Refund date: 2025-10-24T22:39:11.777542

ğŸ’¾ Context: 6 msgs, 2103 tokens (13.1%)
```

### Long Conversation Example (Context Compaction)

After 12+ messages, the system automatically compresses history:

```json
{
  "message": "compaction_started",
  "total_tokens": 12500
}
{
  "message": "messages_summarized",
  "compression_ratio": "73.2%"
}
{
  "message": "compaction_completed",
  "tokens_after": 8230
}
```

User sees:
```
ğŸ’¾ Context: 9 msgs, 8230 tokens (51.4%)
```

**Result**: Conversation can continue indefinitely without context overflow!

---

## ğŸ”¬ Technical Highlights

### 1. Agent Communication Protocol

All agents communicate via standardized Pydantic models:

```python
# Request format (Coordinator â†’ Specialized Agent)
AgentRequest(
    agent="policy_expert",
    task="search_policy",
    context={"query": "refund eligibility"},
    metadata={"session_id": "..."}
)

# Response format (Specialized Agent â†’ Coordinator)
AgentResponse(
    agent="policy_expert",
    status="success",
    result={"policy_text": "..."},
    metadata={"latency_ms": 234, "tokens_used": 120}
)
```

### 2. RAG Pipeline (Async)

```python
async def rag_search_tool(query: str) -> str:
    # 1. Generate embedding (async Vertex AI API)
    query_embedding = await model.get_embeddings_async([query])

    # 2. Retrieve chunks from Firestore (async)
    chunks = await retrieve_policy_chunks_async()

    # 3. Rank by cosine similarity (sync, CPU-bound)
    top_results = rank_by_similarity(query_embedding, chunks, top_k=3)

    # 4. Return concatenated text
    return "\n---\n".join([r["text"] for r in top_results])
```

### 3. Structured LLM Outputs (Pydantic)

Force LLM to return valid JSON matching Pydantic schema:

```python
from pydantic import BaseModel, Field

class IntentClassification(BaseModel):
    intent: Literal["refund", "policy", "general"]
    confidence: float = Field(ge=0.0, le=1.0)

# LLM call with schema enforcement
config = GenerationConfig(
    response_mime_type="application/json",
    response_schema=IntentClassification.model_json_schema()
)

response = await model.generate_content_async(prompt, generation_config=config)
intent = IntentClassification.model_validate_json(response.text)  # âœ… Type-safe
```

### 4. Parallel Execution

```python
# âŒ Sequential (slow)
policy_result = await policy_expert.handle_request(...)  # 500ms
order_result = await transaction_agent.handle_request(...)  # 500ms
# Total: 1000ms

# âœ… Parallel (fast)
results = await asyncio.gather(
    policy_expert.handle_request(...),
    transaction_agent.handle_request(...)
)
# Total: 500ms (max of both)
```

### 5. Conversation History Management (Context Engineering)

**Intelligent context window management for unlimited conversations:**

```python
from src.utils.conversation_history import ConversationHistoryManager

# Initialize with context engineering settings
history_manager = ConversationHistoryManager(
    max_tokens=16000,         # Gemini 2.0 Flash context window
    target_tokens=12000,      # Trigger compaction at 75%
    keep_recent_messages=8,   # Always preserve last 8 messages
    enable_summarization=True # Auto-summarize old messages
)

# Track conversation
history_manager.add_message("user", "What is your refund policy?")
history_manager.add_message("assistant", "Our refund policy allows...")

# Get stats
stats = history_manager.get_stats()
# {'total_messages': 12, 'total_tokens': 4523, 'token_usage_percent': 28.3}

# Get formatted context for LLM
context = history_manager.get_context_for_llm()
```

**Compaction Strategy (Sliding Window)**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ First Message (system context)     â”‚ â† Always preserved
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Middle Messages (10 msgs, 5K tok)  â”‚
â”‚     â†“ Summarization (Gemini)       â”‚
â”‚ Summary (3-4 bullets, 1K tokens)   â”‚ â† 80% compression
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Recent 8 Messages                   â”‚ â† Always preserved
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benefits**:
- âœ… Unlimited conversation length (no context overflow)
- âœ… 70-80% token compression via LLM summarization
- âœ… Real-time monitoring: `ğŸ’¾ Context: 12 msgs, 4523 tokens (28.3%)`
- âœ… Automatic fallback to pruning if summarization fails

---

## ğŸ§ª Testing

Run the test suite:

```bash
# Run all tests with pytest
pytest tests/ -v

# Run specific test modules
python tests/test_system.py              # 3 automated system tests
python tests/test_refund_flow.py         # End-to-end refund flow
python tests/test_multi_agent_system.py  # Multi-agent orchestration
```

**Test Coverage**:
- âœ… Unit tests for each agent (`test_policy_expert.py`, `test_transaction_agent.py`)
- âœ… Integration tests for multi-agent orchestration (`test_multi_agent_system.py`)
- âœ… RAG pipeline tests (`test_async_tools.py`)
- âœ… Pydantic schema validation tests (`test_schemas.py`)
- âœ… Order ID extraction tests (Spanish + English) (`test_coordinator_extraction.py`)
- âœ… Prompt loading tests (`test_prompts.py`)
- âœ… End-to-end refund flow tests (`test_refund_flow.py`)

**Sample Test Scenarios**:
- âœ… Eligible refund (within 14 days, DELIVERED status)
- âœ… Ineligible refund (>14 days)
- âœ… Already refunded order
- âœ… Policy-only queries
- âœ… Spanish input without ORD- prefix (`"pedido nÃºmero 25836"` â†’ `"ORD-25836"`)
- âœ… General queries (out of scope)

---

## ğŸ“Š Design Patterns Implemented

- **Template Method**: `BaseAgent` defines skeleton, subclasses implement `_execute_task()`
- **Factory**: `AgentResponse.create_success()` / `create_error()` helpers
- **Strategy**: Coordinator routes to different agents based on intent
- **Singleton**: Global `settings` instance, logger cache
- **Fail-Fast**: Eligibility checks ordered from fastest to slowest

---

## ğŸ¯ Future Enhancements

- [x] ~~Embeddings cache (LRU) for repeated queries~~ âœ… **Implemented** (33% hit rate)
- [x] ~~Conversation history management~~ âœ… **Implemented** (v1.2.0)
- [x] ~~Context window compaction~~ âœ… **Implemented** (70-80% compression)
- [ ] Pass conversation history to Coordinator for context-aware responses
- [ ] Persist conversation history to Firestore for multi-session continuity
- [ ] Circuit breaker for Firestore failures
- [ ] Human-in-the-loop for high-value refunds (>$500)
- [ ] A/B testing for prompt variations
- [ ] Multi-language support (i18n)
- [ ] Streaming responses for better UX
- [ ] Semantic search over conversation history (embeddings + vector DB)

---

## ğŸ“š References

- [Google ADK Documentation](https://cloud.google.com/adk)
- [Firestore Vector Search](https://cloud.google.com/firestore/docs/vector-search)
- [Pydantic V2 Documentation](https://docs.pydantic.dev/latest/)
- [Langfuse Observability](https://langfuse.com/docs)
- [Context Engineering Improvements](./CONTEXT_ENGINEERING_IMPROVEMENTS.md) - Deep dive into v1.2.0 changes
- [Full Changelog](./CHANGELOG.md) - Complete version history

---

## ğŸ“ License

MIT License - This is a portfolio/learning project demonstrating production-ready AI agent architecture.

---

## ğŸ‘¤ Author

Built as a technical demonstration of:
- Multi-agent systems (A2A communication)
- Production-ready async Python architecture
- RAG implementation with vector databases
- LLM integration best practices
- Cloud-native AI applications
- Context engineering & conversation memory management

**Version**: 1.2.0
**Last Updated**: October 24, 2025

### Key Milestones
- **v1.0.0** (Oct 20, 2025): Initial multi-agent system with RAG
- **v1.1.0** (Oct 24, 2025): Enhanced order ID extraction (multilingual support)
- **v1.2.0** (Oct 24, 2025): **Context engineering & conversation history management**

For questions or collaboration: [Contact Info]
